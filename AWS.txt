OS components are-->shell and kernel
shell-->GUI, CLI, PowerShell which gives user a flexibility to user to interact with hardware

kernel-->hardware--source code
it is considered as the core of system


power shell is mostly used by the developers as it includes developing commands 




application software
OS--> Shell, Kernel
hardware


source code-Kernel
developer develops the source code ex-windows, we cant change it as it is closed--closed source


if u face any trouble like as u purchased word but not working then they will give support as they are responsible
 support--dedicated team--responsible


closed source code means which is closed by the developers it is not free we need to pay to use but we cant change any modifications

example
windows
Unix--mac OS, apple los---raw Unix used by server and modified is purchased by apple

 -----OPEN SOURCE CODE-----

example-Linux
any OS working on OS is opensource OS
Linux--red hat
Linux--android
dedicated support---not available even if its source code 

community support--like creating any application by modifying code as team they support them with ideas but not Linux even if it is free



SERVERS AND CLIENTS:
1.centralized management
2.security
3.Availability(11 9's)
99.99999999999( 1 sec in 5 years)
0 downtime
4.scalability(where servers are increased without human involvement)
5.reliability(the time required to recover from the crash)

13/6/24

--------virtualization--------------
to overcome the challenges faced with physical server--takes more time for setup, prices, no expansion
physical and virtual servers


aec	acet		poly	pharm
10 cores 20	10	 10	 10
8gb	 8	8	 8	 8
vm	 vm	vm	 vm	 vm
hypervisor
rack server - physical
40 gb ram, 4tb hdd, 50 cores CPU


14/6/24


launch Linux virtual machine from AWS and try to do some commands

console home--search--ec2--virtualserverscloud--click on it
come down in instance and click on launch instance--give any name all small letters
select AWS Linux   
come down instance type-don't change 
key pair--click on create a new keypair-then give Ur rollnum--don't change any option and create keypair   

don't change any network or storage or advanced
now right side ---launch settings
after process success
in instances u can see Linux server



to connect Ip address, username, password

///////////////////////////////////////////////

Virtual machine launch------
Linux—VM—Launch—for—AWS
Sandbox -AWS
S1:- console home 
S2:- go to search bar and search for ec2 
S3:- click on ec2(virtual services in cloud)
S4:-come down and launch instance
S5:- on launch instance give any name in lowercase
S6:- use amazon Linux and go down
S7:- create keypair-give your roll number and come down
S8:-create key pair. We can see keypair is downloaded
S9:- come down to see storage you can see a launch instance on right
S10:- after you see success, click on instance on above and click on it to see Linux server is created

mobaXterm
S1: section
S2:-ssh
We need Ip address, username, password
S3:-Go to AWS instance select it
S4:- you will get details on down side 
You have public ipv4 Ip address copy it
S5:- paste at remote host
Go to AWS again click on connect button after selecting in instance
And select ssh client and copy the thing between “ and @ it is the user name
S6:- now copy it in specify user in mobaXterm
Go to advanced setting in mobaXterm
And go to downloads for the keypair which we downloaded from use private key.


bash-bourne again shell

-----Types of shells-----

k shell     
z shell       
c shell      
sh-shell      
bash- bourne again shell 

$--means a standard user account
#--means a root account


-----Basic Linux commands----

1.sudo su
~"#" represents a root user/account
2.pwd 
(Print working directory)
3.ls 
list of items in a file
4. exit 
exits from root account to standard account
5.cat > filename 
creates a text file
Enters the text file in that
for saving this txt file
~ also used to replace the content of the existing file
6.ctrl+z 
for saving file
7.cat >> filename 
adds the extra information into the existing file
8.cat filename 
shows the content in the file
8.ls -l 
gives the output of the file even more
9.ls -la(important command) 
gives the information about the existing files along with that also shows the hidden files/directories created by OS itself . The user doesn't have the direct access to this 
output which starts with the 'd' is a directory and the other are files
10.clear 
clears the screen
11.mkdir 
make a directory
12.cd directory_name 
change to directory
13.cd ..  
step down to one directory to the previous directory
14.cd path of the new directory
changes directly to the given path directory
15.rm---to delete a file
16.rm -r dirname-----delete directory here r--means recursive
17.rm -rf dirname---deletes directory directly
18.man command----to know information about the command

19.touch filename 
used to create a empty file ,at a time we can create multiple files
and also used to change the timestamp of the file creation

20.cp filename destination(directory) 
Ex: cp file1 dir1 
to copy the file 
Also --- 
cp filename(source) destination path
Ex: cp file10 /home/ec2-user/dir1

21.cat file_path 
opens the file in the given path

22.mv filename new filename(destination file) 
it is a cut paste concept

23.history 
to see the commands we used (history of used commands)

24.uname -r (important command) 
gives the kernel version 

25.yum update 
it is software i.e. is updates the software


----Package management tools in Linux(most commonly used)::
1.apt(available for ubuntu)
2.yum(Amazon Linux only use this tool)
 
----Editing tools available in Linux:: 
vi - obsolete -older(for every action we need a command for this tool)
vim -latest(vi updated to vim, flexible than vi)
nano -latest

25.vim filename
used to create a new file and edit (OR) opens a existing file and edit

vim has two modes 
1.read only mode
2.insert mode
but it by default it is in read only mode
to edit the info in file we need to press 'i' on the keyboard. Otherwise it can't be edited. in insert mode file can't be saved .we need to go back to the read only mode by pressing 'esc' on keyboard. then to save we need to press ':wq'(write-quit)
there are shortcuts available in vim like notepad.
to come out of the file without saving the file we need to press ':q!'(!--forcefully closes the file)

26.vimtutor
to learn about vim tool
 
27.nano filename
it doesn't have any modes it directly goes to insert mode
by pressing 'ctrl+ o' we can write and 
'ctrl+ x' we can exit the file 





----Linux file hierarchy----

stage1		/
		|
stage2  bin boot dev etc. home root ......
                  |
stage3	Alice bob ...other users

*stage 1 and stage 2 are completely protected where standard users(stage 3) can't use the / and other directories in the upper stages> it is not visible for the users.
*The upper stages can control the lower stage users
*the standard users can't elevate to the root user until the root user gives the permission




18/6/24


etc--brain of the OS
etc contains all the config files which is like all settings 


28.ls -l | more----gives only few directories and shows more if u want to see all then go for enter or ctrl+c to stop the process as it kills any running process in Linux
29.vim passwd-- a brief info about user  ':qa' to exit it
30. vim shadow or  cat /etc/shadow--- gives brief info about password requirement of any user 
31.cat /etc/group--- gives which group the user is belong
32.date ---gives current date
33.cal--calendar
34.cal month--calendar of that month
35.timedatectl---gives time zone including the date 
36.head filename--gives first ten lines of the file
37.tail filename---gives last ten lines of the file
38.head -n 5 passwd--first 5 lines
39.tail -n 3 passwd--last 3 lines
40.useradd username--adds the user
41.whoami-- gives with which user u logged in 
42.if to want to log in as bob then-- su bob--su username
43.passwd is the command used to create a password for the user--passwd username

only root can create a password for the user no user can create cause there are no permissions
root can switch to any user but user cant switch to any user
44.userdel username--deletes the user



19/6/24
-------------------SERVER--------------------

based on the application we install OS server 

4.user(design)
3.applictaion(webserver application ) Apache, node, react, google flutter
2.OS(server version)
1.hardware(memory ,storage, CPU)

it is user responsibility to design server in what way they want


=>if client specify the below conditions
website--Linux--ubuntu
cloudkitchen.com---website name


4.content(cloud kitchen)----the customer i.e. client will design what  matter they wanted
3.weserver application
2.ubuntu server latest version
1.hardware(16gb,2tb,10 cores)

it is cloud server responsibility i.e. AWS to setup 1,2,3 steps

=>email server

4.emails
3.application email server
2.os server version
1.hardware

=> Ip server

4.ip's
3.ip server application
2.os server
1.hardware


=>lms server
lms(learning management system like Udemy, etc.)

4.content
3.lms application(Moodle)
2.os server
1.hardware


=>amazon prime
4.videos
3.ott application
2.os
1.hardware

physical--on premise 
virtual--cloud server


f/d   user  group  others
-     rwx   rwx    rwx
-     rw-   rw-    r--

where,
r-->read permission 
w-->write permission
'-'--> no permission
x --> execute 
- --> file(in f)
d -->directory

Ex: scenario for salary of employees 

dept    users groups  others
account ---   r--     ---
hr      rwx   rwx     ---
mrktg   ---   ---     ---



chmod--used to change the permissions
ex-chmod u-w 
u----user
'-'-----remove
'+'----add 
w---which permission u wanted to delete or add


20/6/24
--------------------CLOUD------------------------
cloud service providers(csp)
AWS
azure
gcloud
oracle cloud

nist---national institute of standards and technology
gives standards to cyber security products, electronic devices
standards means some conditions need to be satisfied by nist

5 characteristics of cloud

=>on-demand self service
       means whenever u have a demand that u need a server u can go and launch server by logging in, no need to contact  
         like we are launching Linux servers to practice
=>broad network access
         means we are accessing server by connecting it using public Ip address in which public means u can use wherever
         u want in the world
         the datacenters are connecting to the existing internet
=>resource pooling
         by predicting the information like how many people want to deploy servers and how much resources they need, the
         datacenters
         are pooling the resources without making resources exhausted, this can be done by the datacenters,
         Microsoft has build the underwater data centers, cause no government provide access to make harm to greenery,
         etc..    
         
=>elasticity
         ability to increase/decrease the resources
            
=>measured services(metered services)
         renting like u have to pay for using the limited 
            
operating system
server-client
virtualization
Linux
cloud computing

IaaS--infrastructure as a service
PaaS--platform
SaaS--software




21/6/24
we know that chmod is for changing the permissions 
we can also change by using the numeric 
r--4
w--2
x--1
chmod 777 test---this is the most dangerous cause it gives permissions to everyone.
chmod 600 test--- gives permissions to the read and write to user only 
chmod 644 test-- gives read and write permissions to the user and read    
                 permissions for the group and the others

------------cloud deployment models----------
4 types of deployment models
1.private cloud
2.public cloud
3.community cloud---accessing resources by paying rent ex USA government
4.hybrid cloud--combo of private and public 

-----------------------AWS----------------------

AWS is maintaining the global infrastructure
=>AWS regions

        region means a geographical location, not based on the specific county
        under the region we have availability zones 
        there should be more than 2 availability zones to become a region

=>AWS availability zones
        there should be more than 2 data centers to become a availability zone

=>AWS data centers

regions--->AZs--->datacenters---->physical resources---->our data

##AWs global infrastructure----website to know more about AWS


22/6/24
regions, azs, dcs are 
connected theoretically---->

low latency
high bandwidth
highly available
fiber optic cable

virtual machine----means virtual server
in AWS ec2 is virtual server
EC2 instance--ECC-elastic compute cloud


()
---------------putty---------------------
session open---u can see host name paste the Ip address
then select (ssh--secured shell)
now come to the ssh in left side ---dropdown--then u see authentication--dropdown---select credentials--now paste the key downloaded

4 way of connecting is to go for path where u have created the keypair and type cmd
now type ssh -i ubuntu/Linux/server u selected@public ip address


for windows type sever launching we use windows OS and change the version only take general versions not core type as it is related to the cmd

now its protocol is rdp-- remote desktop protocol

---------------------//-----------------------------

To lunch windows--------in mstsc
Windows protocol is rdp(remote desktop protocol)-3389 is server protocol number of rdp.
Don’t go to core while choosing instance choose base.
Username-----we get it in connect and go down. It is Administrator
Passwd—we have click on get passwd. And need to upload the file of downloaded virtual machine. Then decrypt that.
Now we have a tool (by default software)– ms terminal service client
Short cut is mstsc, search it on run.
Paste public Ip. Connect
Go to more choices –use a different account
Give username-Administrator and password and connect and click on okay.



----------------------------//---------------------------
For copying files ini windows:-
in mobaxterm:-

session->RDP->Ip address, Password->Advanced RDP setting->local resources->Redirect drives->ok

in mstsc:-

more options->local resources->more->drivers



24/6/24
--------------------copying files in Linux------------------

ssh--22
sftp-secured file transfer protocol 22 as it is the subshell of the ssh




----------------------deployment of website into server----------------------

data-content
enable security group with port num
webserver application--yum install httpd
os-amazon linux
hardware


port number acts as the door if it is 22 then it says connect to ssh service only
=>after launching virtual machine need do updates by using yum update.
=>then type yum install httpd.---http-hypertext transmitting protocol
security group level rules only applied to ec2 rules.
=> the type systemctl start httpd
=> go to the security click on security rules and go to inbound rules now click on edit inbound rules and add http and enable portnumber 

now goto mobaxterm and type cd /usr/share/httpd/noindex/
 now ls -l----->index.html file will be appeared
now cat index.html it shows by default it woks as we saw in the browser
now vim index.html and now u can edit that like manasa or anything u wanted
now again u go to browser where u have pasted public ip address and refresh what u have written in the file is displayed




=======================ABSENT CLASSS===============================
25/6/24
----------------------deployment of website into server----------------------

data-content
/var/www/html/index.html
sample webpage-public ip
systemctl start apache2
enable security group with port num 80
apache2--(apt-get install apache2)
apt-get update
os-ubuntu
hardware



In ubuntu

components needed
> data - content
/var/www/html
then open -- index.html file and edit it
sample webpage - public ip 
> security group -- port no. 80 -- protocol httpd
'systemctl start apache2'html/index.
> webserver application 
install apache2 (command -- apt-get install apache2)
before apache2 install we need to upgrade the system
command -- 'apt-get update'
> webserver OS (we are using ubuntu here)
> hardware


========================================================================
 
   ---------Protocols & port no. for remote connection ------------
> RDP(Remote desktop protocol) -- '3389'  -- only used for windows environment(connection is secured / encrypted)
> SSH(Secure shell) -- 22 -- used for Linux version can't be used for windows (Encrypted connection)
> TELNET -- 23 -- Linux unsecured connection (Plain text)

------------- Other connections---------------
these are used to transfer files
 > SFTP (Secure file transfer protocol) -- '22' -- encrypted file transfer
 > FTP (file transfer protocol) -- '21' -- not a secured one / plain transfer 
 > HTTP(Hypertext transfer protocol) -- '80' -- not a secured(unencrypted)
 > HTTPS(Hypertext transfer protocol secure) -- '443' --secured transfer


===========================================================================
 ~~~~~~~~~~~~~ web application deployment ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
************ File transfer for web application IN UBUNTU

> give permissions to all the users, groups and others by giving command 'chmod 777 /var/www/html' 
> open sftp for the ubuntu and change to the direction to the above path (/var/www/html) in cmd of the sftp 
> delete the existing index.html file 
> now drag and drop the folder/directory 
Sample: Coffee shop directory
click on coffee shop directory and we can see two files assets and index.html 
first drag and drop the assets directory/folder
and then drag and drop the index.html
> now start the web server by using command 
'systemctl start apache2'
> refresh the page we can get the downloaded/import web application.




25/6/24
-------------------------------------------------------------------------
remote connection protocols
1 is rdp-remote desktop protocol-3389-it works on windows environment
this is completely secured(secured id nothing but encrypted)
2 is ssh-secured shell-22(all Linux and Unix based environment use ssh)
this is also more secure 
3 telnet-23-it supports Linux-it is a unsecured-unencrypted connection
any one can easily read 
these are the remote connections
transfer protocols
sftp-transfer the file from your location to another location
secured file transfer protocol-22
ftp-21 this is not secure (not recommended)
website protocols
http-80-not secured unencrypted-(it is used for website)
https-it is more secure-hyper text transfer protocol secure-443

web deployment using ubuntu
    1  apt-get update
    2  apt-get install apache2
    3  systemctl start apache2
    4  cd /var/www/html
    5  ls
    6  vim index.html

26/6/24

nginx----->path---->/usr/share/nginx/html/---to start 
same as apache2 but 
-->apt update
-->apt install nginx -y
-->systemctl start nginx
-->systemctl enable nginx
then go to /var/www/html give access
then delete index file now drag and drop the whatever website u wanted to



-------------WINDOWS-----------------------
1: launch windows in remote desktop connection
2: open server manager close the pop up
3: Add roles and features don’t change any thing
4: In roles we need to select web server(iis-internet information service) in checkbox
5: install
6: after installation-close server manager
7: go check the website
8: note down the location of folder inetpub open it you see wwwroot and you see issstart htmlpage
9: you can write your own on notepad. Then save as all files with name index.html and close it. You can see it is created. 
10: now refresh it
==================================================
stop and start instance then load Ip addressed page then reload can be seen cause ip address been changed.
elastic Ip----->permanent Ip address--chargeable

========================================
allocating elastic Ip address
->select elastic Ip address-->select allocate-->allocate-->select associate this ip address
->select for which server u wanted to allocate-->select and associate



27/6/24



===========================================================================
after selecting the instance ex ubuntu and creating the keypair before launching 
=>go to network settings
=>change availability zone to 1d
=>now in security group--go for edit and change to http
=>now go for advanced details and give the script over there which is below the choose file
    |
    |
#!bin/bash
sudo su
apt-get update -y -----by giving -y no need to involve in that process as it asks y/n if u don't give 'y' there 
apt-get install apache2 -y
systemctl start apache2

------------------------
init 6-restart
init 0-shutdown
--------------------------------------------------------

we usually stop and start instances but sometimes it may cause a lot damage
hence we enable double confirmation like click on the server which u wanted to change settings and 
actions--->instances settings--->choose which one u wanted to enable like stop/terminate--->enable


now if u stop server it wont be stopped it asks u to modify that settings first
============================================
~~~ Changing the index.html file and editing with other ~~~
IN UBUNTU

> open instances and launch instances as above editing's
#!/bin/bash ---- gives the command to go to bash 
sudo su
apt-get update -y 
apt install apache2 -y
systemctl install apache2

> enable the permissions for users
chmod 777 /var/www/html
 
> now remove the index.html file 
rm /var/www/html/index.html

> create an empty index.html file using touch command
touch /var/www/html/index.html

> use echo(command used to print)
syntax: echo "content to be printed" > filename where content need to be sent/path
echo "this is technical hub" > var/www/html/index.html

> save changes and copy Ip address and open in browser


SIMPLE PATH FOR ABOVE PROCESS:

instances -> 

**EC2 is a IaaS


=======================================================
		PLATFORM AS A SERVICE
=======================================================


* AWS Elastic Beanstalk is a PaaS
* It handles the deployment
* An easy way to get web application up and running
* No additional charges for Elastic Beanstalk
* It is a managed service that automatically handles
> Deployment
> Load balancing
> Health monitoring

~~ Benefits
> fast and simple to start using
> difficult to outgrow
> complete resource control
> developer productivity

~~~~ Launching Beanstalk in AWS academy

Steps to launch instances in Beanstalk

> go to sandbox environment and start the lab
> Go to console home and search Elastic Beanstalk
> then click on create application and select web server environment 
> give the application name . by this env will be added by default. Ex:My-webserver the environment file will be - My-webserver-env 
> Come to domain, now give a domain name
> to check if the domain is available or not click on 'check availability'
> In platform give the platform type as 'Tomcat' and don't change the default changes made by it.
> Go to Application code ---- click Upload your code then give any version label (ur wish) click local file 
> application upload
select all related to the application.
For example:
coffee shop website application:
open the folder and select all the files by clicking ctrl+A then right click select compress to Zip file.
> In Presets Click the High availability (For root account)
> In Presets Click the High availability (For student account)
> Click next
> In service role use default one 'Use an existing service role' in that select 'Ec2InstanceRole' and also for EC2 key pair select the default key 'vockey'
for EC2 instance profile select 'LabInstanceProfile'
> Click next
> In VPC don't select the work VPC
> Come to Instance Subnets select the availability zones any no. of zones
> Scroll down and click next
> Scroll down to EC2 security groups and click default
and click next
> Again scroll down to click next
> submit
Note: You can check if the instance created or not in 
AWS console -> search EC2 --> instances


1/7/24
===============================================================================
AWS VPC-VIRTUAL PRIVATE CLOUD---it is a network service
private cloud means ---creating the private network that we can deploy our resources and get connected


-------------------NETWORKING------------------------------
networking means communication
protocol=set of rules and regulations
one to one--unicasting, one to group---multicasting, one to all--broad casting
address---MAC address, IPv4 address, IPv6 address
IP addressing--identity 

cmd--ipconfig----windows computer--to see the Ip address


Ip addr show---ubuntu computer

ipconfig----specially works in a Linux machines


Ip address we are getting is tracked by the IANA---INTERNET ASSIGNED NUMBERS AUTHORITY

IANA--which gives IP address
IPv4 address-4+million--2023 all IPv4 are finished but all of the world is surviving with IPv6
IPv6----in background communicated with IPv4 but we don't know IPv6 address



MAC---MEDIA ACCESS CONTROL also known physical address

MAC address will not be changed but IP can by Which network u are connecting or what address the network administer wanted to give
getmac----we can know the mac address of the every interface we have in our laptop


ipconfig /all----to see the more information about the interfaces likes services running ,physical and IP address of the interfaces.


ping google.com---- to check the network status between the our laptop and google network
source is automatically our device and destination is what network u wanted to check.
ping will check the connection only but it doesn't connect to that network

tracert----trace root---it'll give through how many servers we are connected to that website 


ec2--launch Linux/windows and try to ping Ip address---but it ill not give 
the connect with ssh and try to ping--it'll not reply 
now u need to add a protocol -icmp in security rules and then ping it'll reply

icmp----internet control messaging protocol

IPv4 address---unique
ex:10.44.23.54-4parts
octate-4 octates
1 octate=8bits
therefore 32 bits
IPv4=32 bit binary system

we can put minimum number 0 and maximum number 255 in IPv4
0000000.00000000.00000000.00000000
0.0.0.0

> No two devices consists same Ip address and should not be shared
>No IP address will start with '0'
> Ip address should be unique to every individual device
> the Ip address is divided using  '.'(dot) and each part is called octate consisting 8bits for each octate
Ex: 10.16.54.156 -4 octates(parts)
4 x 8 bits= 32 bits
> IPv4 - 32 bit binary system
> minimum bit used in a IPv4 is --> 0.0.0.0
> the IPv4 IP positional formula -2^n
Ex: 11111111.11111111.11111111.11111111

Consider the first octate(part)
octate         1    1    1    1    1    1    1    1
position       7    6    5    4    3    2    1    0
binary	      2^7  2^6  2^5  2^4  2^3  2^2  2^1  2^0
binary value  128   64   32   16   8    4    2    1

> the largest bit value that can be used in IPv4 is 
  255.255.255.255


According to IANA IPv4 is of 5 classes
A->0-127
B->128-191
C->192-223
D->240-255

=>Network means multiple connections of devices
LAM-local area network

gateway -another name-router

A class--N.H.H.H
N--network-one which is not changing-identification
H-host-one which is keep on changing      
In A class the first octant should not change and 2nd , 3rd, 4th will change
EX:BILLGATES----101,102,103
          |          |
      NETWORK      HOST                   







B(128-191 ,format-N.N.H.H ,128.0.0.0 to 128.0.255.255, 128.1.0.0 to 128.1.255.255,129.0.0.0 t0 129.0.255.255,..........191.0.0.0 to 191.255.255.255 )
	C(192-223, format-N.N.N.H,192.0.0.0 to 192.0.0.255 ,192.0.1.0 to 192.0.1.255 ,.........223.0.0.0 to 223.255.255.255)
	D(224-239)
	E(240-255)
>>No Ip address will start with the 0.
>>Network refers to the multiple connections of devices.
>>The exit and entry point in network is called gateway/router.
>>combination of two or more networks form internet.
>>within the one network forms intranet.

Examples:-----
last Ip=223.255.255.255
last network=223.255.255
3rd network in  c class=192.0.2
last Ip in 3rd =192.0.2.255
netwoks-3,host-256

10th network in A class:-10.0.0.0
260th Ip address in 10th network:-10.0.1.3
number of hosts in 10th network(1,67,77,216)(256*256*256)
number of networks in A class-127


5th network in B class:-128.4.0.0
256th Ip address in 5th network:-128.4.0.255
number of hosts in 5th network(256*256=65536)
number of networks in B class 64*256=16384                                                                                                                                                                                                                                                                                                                                                                                                                                       




~~~~~~~~ RESERVED IP ADDRESS
> These are the Ip addresses which are permanent but not recommended to use
> unicasting IP address 1-223 (in A,B,C classes)
> multicasting IP address range --> 224-239 --> which general computers don't allow. Ex: set-up box in houses consists this IP address, Routers.(in D class)
> R & D purpose (special usage) --> 240-255 (in E class)
Ex: Used by Research and Development institutions like  IEEE, IETF,IANA,EIA so on,..
> General people can't use D,E classes. They can only access A,B,C classes only
> the 127 range is called the loopback. We should not use 
127.0.0.0 the complete range shouldn't be used. Loopback is also called as Always active connection. It is by default present in your PC.



RESERVED IP ADDRESS:
>>>a, b, c have the range from "1-223" and follows "unicast" communication.
>d -- 224-239 -multicasting --EG: tata sky
>e -- 240-255 - r&d(research and development) ,special used by the standard institutions -- standard institutions used this "e" class as  special
purpose in electronic network area. 
[ieee, ietf, iana, eia ---standard institutions]
>OS -- 169.254 -- APIPA


-----------------------------------------------------
>A B C reserved
1.A class didn't start with 0 , and has the range to 127 called "loop back" or "always active connection".
  1.0.0.0 --- 127.255.255.255



B class--->128-191
169.254.0.0-----169.254.255.255
APIPA--AUTOMATIC PRIVATE IP ADDRESSING
2.B class -- 128-191 -- N.N.H.H
  169.254.0.0 --- 169.254.255.255 
>APIPA = Automatic Private Ip Addressing :The range from 169.254 is reserved for apipa.
>APIPA  comes from DHCP[dynamic host configuration] is also called as Ip server.
>DHCP server is giving Ip address to the mobile ,laptop .it is a small service or server that automatically enables the Ip address when ever the network is turned on.so this type of assigning Ip address is called automatic assigning. 
>protocol -- server - IP service.
>If there is any issue or shutdown in the DHCP server then APIPA comes into picture.
>In the absence of DHCP server ,and there is no Ip address from the laptop user then the laptop requests the OS then the OS temporarily generates the Ip address to the laptop. This process is called automated private Ip address.
>OS -- 169.254 -- APIPA


Connecting the DHCP server to the pcs or laptop:
1.open packet tracer 
2.take one server and two laptops on the screen
3.now choose server then click services --- DHCP --- service on --- start Ip Eg:1.0.0.2 -- subnet mask Eg:255.0.0.0[fixed] --- save
4.now again choose desktop --- give Ip address for the server other than the Ip that we given in the services 
5.Now choose the pcs or laptops --- desktop --- select DHCP --- automatically assigned because the Ip that we given in the services is assigned to the clients.
6.If the DHCP service is off then the clients get the temporary Ip address that was assigned by the OS is called APIPA.




3/7/24
=================================================================================================
RESERVED IP cls-2
1. network id--first Ip 
2.broadcast id---last Ip

every first Ip in every network is the network id and that id we can't use
EX:162.90.54.23---is an Ip in  a network of 162.90.54 as it is 'C' class where its first Ip is 162.90.54.0---is a network id


every last Ip in every network is the broadcast id and that id we can't use
EX:162.90.54.23---is an Ip in  a network of 162.90.54 as it is 'C' class where its last Ip is 162.90.54.255---is a network id

>consider 192.34.8.0
there will be 256 Ip address in this network but only 254 are valid Ip addresses

10.250.67.89-----in this network the following are
10-network id
10.0.0.1--first Ip -network id
10.255.255.255-last Ip - broadcast id
10.255.255.254-last usable Ip
10.0.1.0-256 Ip address in this network



After removing all the reserved IPs the leftover Ip's are again divided into two categories
1.private Ip-internally, privately
    ->rfc1918 is the standard that given by IANA

2.public Ip- ISPs, internet IPs

> Broadcast IP . network IP --> Reserved IP

~~~~~~~~~~~~~~~~~ SUBNET MASK/ PREFIX LENGTH ~~~~~~~~~~~~~~~~~~~

> Other types of IP 1.Public IP
		    2.Private IP

> private IP -> standard that given by IANA -- rfc1918
> According to IANA private Ip addresses should be used internally or privately by the organizations
> According to IANA public Ip addresses should be used by Cloud providers and ISPs, internet IPs
> To connect with internet we should get a public IP address
> epabx system- internally IP address can be used but publicly we cannot use that IP address --> also called Intercom(Inter communication)

============ RANGE OF PRIVATE IP =========================
> RFC1918 -> Private IP Address Range
> we can use this private Ip's to connect internally with
free of cost
10.0.0.0 - 10.255.255.255.255
172.16.0.0 - 172.31.255.255
192.168.0.0 - 192.168.255.255
==========================================================






----------------------------SUBNETTING-------------------

used to reduce the wastage of Ip address and to increase the security and reduce broadcasting
~~~~~~~~~~~~~~~~~~~~~~~~~~~~

>Default Subnet mask for A,B,C classes

A class - 255.0.0.0 - N.H.H.H
B class - 255.255.0.0 - N.N.H.H
C class - 255.255.255.0 - N.N.N.H

> These default subnet masks help the system in which class the given IP belongs to
Subnet mask :[used by windows]
>A class --- 255.0.0.0 [256*256*256--connections -- i.e. Ip addresses]
>B class --- 255.255.0.0 [256*256--connections -- i.e. Ip addresses]
>C class --- 255.255.255.0 [256--connections --i.e. Ip addresses]


prefix length:[
we have 8 1's in one octate.
1.A class -- 255.0.0.0 -- /8 [no of 1's in 255 octate] network bits [only 1's]
2.B class -- 255.255.0.0 -- /16 [no of 1's in 255.255 octate] network bits [only 1's]
3.C class -- 255.255.255.0 -- /24 [no of 1's in 255.255.255 octate] network bits [only 1's]

** / or 1 represents network bits
** 0 represents host bit

>>classful Ip addressing - default subnet mask or default prefix length.
>>computers will understand both subnet mask and prefix length.
Eg: small company -- 30 people -- 30 computers -- we can choose "C" class because wastage of Ip's will be less .
Eg: small company -- 300 people -- 300 computers -- we can choose "B" class because wastage of Ip's will be more when compared to "C" class. 


>>>Subnetting:[splitting of the big network into smaller subnetworks then we reduce the wastage of Ip address]
1.The  main thing to do with the subnetting is reduce wastage of Ip address.
2.increased security
3.reduced broadcast.

**Steps to do subnetting:
Here we have 256 Ip's in a big network -- C class
EG: small company -- 30 people -- 30 computers
  



  
1.requirements -- plan how many Ip address we need -- 30 Ip's --

2.identifying the nearest value by 2^n formula. -- 2^h -- 
2^5 -- 32

3.Writing the new subnet mask.
>considering the default subnet mask that belongs to C class
255.255.255.0 which gives 256 Ip's  -- /24
>255.255.255.00000000
>255.255.255.11100000 -- here we need only 32 Ip's so from 
2^5 we take 0's equal to the power from left and remaining are replaced with networks i.e 1.
>255.255.255.224 -- new subnet mask  -- /27

4.
>no. of hosts  -- 2^h -- 2^5 --32
>no. of subnetworks -- 2^n -- 2^3 --8 [where n =no of 1's that we replaced in the formation of new subnet mask]

5.Writing the range -- 2^h --32
192.168.10.0   - 192.168.10.31  [0-31=32]--[32+32=64]
192.168.10.32  - 192.168.10.63
192.168.10.64  - 192.168.10.95 
192.168.10.96  - 192.168.10.127
192.168.10.128  - 192.168.10.159 
192.168.10.160 - 192.168.10.191
192.168.10.192 - 192.168.10.223
192.168.10.224 - 192.168.10.255

**Eg:
192.168.10.32  - 192.168.10.63
 
1.192.168.10.33  -- first usable Ip
2.192.168.10.32  -- network id
3.192.168.10.63  -- broadcast id
4.192.168.10.62  -- last usable Ip






EX-2:
10 requirements
1.2^4=16
2.225.225.225.00000000---subnet mask
as 2^4 keep 4 zeros aside from right
225.225.225.11110000 -new subnet mask   -/28
3. no of hosts--2^h--2^4=16
   no of networks--2^n--2^4=16
4. writing range --2^h--16
192.168.10.0   -192.168.15
192.168.10.16  -192.168.10.31
192.168.10.32  -192.168.10.47
192.168.10.48  -192.168.10.63
192.168.10.64  -192.168.10.79
192.168.10.80  -192.168.10.95
192.168.10.96  -192.168.10.111
192.168.10.112 -192.168.10.127
192.168.10.128 -192.168.10.159
192.168.10.160 -192.168.10.175
192.168.10.176 -192.168.10.191
192.168.10.192 -192.168.10.207
192.168.10.208 -192.168.10.223
192.168.10.224 -192.168.10.239
192.168.10.240 -192.168.10.255
============================ONLINE CLASS=============================

subnets-subnetting
req=1000

req=1000

2^10

255.255.00000000.00000000
new subnet
255.255.11111100.00000000/22
no of hosts-2^h-->2^10=1024
no of networks-->2^n=96-2^6=64
255.255.252.0/22
writing range
134.10.0.0 --> 134.10.3.255
134.10.4.0 - 134.10.7.255
134.10.8.0 - 134.10.11.255

r10:17 AM 9/22/20245
5/7/24
==============================================================================

writing above by calculating like adding 256 per every time means some what tough so formula is to calculate block value is 2^h/256
ex: 1024/256=4 by using block value u can easily write subnetting range like by adding 4 for every range u can write 

EX:2000 requirement 
255.255.00000000.00000000
no of hosts =2^11=2048
no of networks=2^5=32
new subnet=255.255.11111000.00000000/21
255.255.248.0/21
writing range 
134.10.0.0-134.10.7.255
134.10.8.0-134.10.15.255
134.10.9.0-134.10.23.255
134.10.24.0-134.10.31.255
134.10.32.0-134.10.39.255


based on requirements the class we can take are
<256---c-class
>256-<65536---b-class
>65536---a-class


>>>
> If req <256 --> c class (256 hosts)
> If 256< req <65536 --> b class (256*256 hosts)
> If 65536< req <1,67,77,216 --> a class (256*256*256 hosts)

> Ex: 
Enter the last valid host on the network 10.158.32.0/20:


================================================================
 subnettingquestions.com --> for subnetting quiz
================================================================

> Network in cloud - AWS VPC
> virtual private cloud(VPC) - network service in AWS


>>Task
> enable icmp
> connect to ssh
> VPC - internet gateway
> internet gateway of default VPC - detach connect ssh
>>

> VPC is a big network --> out of which we need to create subnets
> the virtual private cloud consists of a gateway in order to let internet to communicate with a EC2 instance
> By detaching the VPC the connection between the ec2 instance and internet 
> As a student account we cannot detach a VPC which is by default exist
> So we need to create new VPC and detach it
~~ Steps to create VPC:
 > Go to search bar and Search for VPC
 > On left-side click on your VPCs and click 'Create VPC'
 > Add a name to your new VPC and Give an IP address between the subnets /16 - /28 
 > Leave the default settings as it is
 > click on 'Create VPC'


~~ Steps to create Subnet for VPC:
 > Below the 'Your VPCs' click on 'Subnets'
 > Click on 'Create Subnet' 
 > select newly created VPC and give IPv4 subnet (IP address) so that we get the number of IPs
 Ex: 192.168.11.63/26 -- 64IPs
 > Click on 'Create Subnet' we get the new subnet

~~~ Launching the Instance using the created subnet and VPC
> Created VPC and Subnet
> Open EC2 and launch new instance
> In VPC, Select the created VPC and automatically subnet will be selected
> Click 'Enable' on Auto-public IP
> Edit security group rules --> add security group rule --> select 'ALL ICMP-IPv4' and '0.0.0.0/0'
> launch the instance
> Now open the command prompt in your laptop 
> type command 'ping instance-public-IP -t'

-------------------------------------------------
 ~~~~~~~~~creating internal gateway~~~~~~~~~~~~
--------------------------------------------------
1.Attaching VPC to internet gate way.
2.go to VPC -- select internet gateway --- create --name it --- create igw --- select the VPC that you created before.
3.the internet gateway is created and the created one is in detached mode.
4.now click the internet gateways in the top and select the gateway that u created and click actions and click attach gateway.
5.now the internal gateway is created and attached to the VPC that we created.

-----------------------------------------------------------------
~~~~~~~~~~~~~~~~~~~~~~~CREATING THE ROUTER~~~~~~~~~~~~~~~~~~~~~~
---------------------------------------------------------------

Go to route tables and click on create the route table
=>now give the route table name and select the VPC which u have created
=>and click on route table
=>now go to route tables and click on the route table which u have created
=>rowting table -- name --- select the VPC that we created before -- create rowte table -- subnet associations -- edit subnet associations -- select the subnet that we created before -- save associations.

=>at below u can see routes and edit routes--add routes and give 0.0.0.0/0 select internet gateway and select the igw and save changes



before we have created and connected VPC and gateways and routers but we are unable to ping with public ip of the instance because internet is able to connect with instance but unable to ping cause the instance doesn't know the path to the internet


------------------subnetting questions.com------------for practice questions for interview use this website


8/7/24
================================================================
			STORAGE SERVICES
================================================================

> data should be durable and consists of 11 9s
> forms of data : 1. data at rest --> saving and storing at a place but not using it - Ex: photos in 			   gallery
		  2. data in transit --> transferring from source to destination Ex: sharing info 		             through WhatsApp
		  3. data in process --> the data in state of execution i.e., still working  and           storing       data Ex: Notepad
 
> floppies, cd, DVD, pen drives, tape drives --> older days storage devices --> temporary storage
> HDD - Hard disk drive -> older hard disk in PC --> permanent 
> SSD - Solid state Drive -> modern hard disk in PC -->permanent 
> HDD - mechanical technology --> less durable
> SSD - semi-conductor technology -->IC Chips --> more durable
--> IOPS (the time takes for retrieving and storing data in SSD)INPUT OUTPUT OPERATIONS PER SECOND
> the more IOPS -- more durability 
> the con of SSD - the chances of recovery of deleted data is less
> the more no. of IOPS - the more u get charged(amount to pay)
> based on purpose Storage devices are divided in two types:
    	1. Block storage - Ex: Internal storages
	2. object storage - Ex: External storages
> the main difference between block and object storages 
   If we have a one-bit of information in a file the block storage consists of block and opens particular block to update whereas in object storage the entire file will be updated. the entire file will not be opened in block storage 
> the block storage is faster than object storage
> this is the reason we use Block storage for EC2 instance
> Block storage - EC2 instances - OS - boot faster - block storage we connect is AWS EBS -> Elastic Block store
> Object storage - External storage - Ex: AWS S3



~~~~~~~~~~~~~~~~
> ADDING STORAGE ON INSTANCE
Steps :
 > Open AWS cloud sandbox -> launch lab
 > Launch instance with windows latest 
 > open mobaXterm and launch windows using RDP
 > Go to AWS and in left panel Scroll down to Elastic Block store -> click 'Volumes' -> create volume -> don't select the GB more as a student account we can't have access to do that
 > Create volume in the same availability zone where EC2
instance launched 
 > Select the volume and in actions click 'Attach volume' and then select the instance u created and select any device name and click 'Attach'
 > open mobaXterm and windows instance --> open command prompt and type 'diskmgmt.msc'
 > you can see the created volume -> right click on offline and
select online 
 > now right click on 'not initialized' and select initialize disk and click next 
 > now right click on unallocated -> select 'new simple volume' -> click next on everything and click finish
 > you can see the newly created volume on it




9/7/24
=============================================================================================

SNAPSHOT----BACKUP


static website is the one where we cant access like login etc. but we can know the information it doesn't need database server EX: technicalhub.io .
dynamic website is the one where we can access like login etc. like it needs database server .

application--->database server.
 Facebook----->credentials like username password.
 Facebook website is opens to every one like it is application server but once u created the credentials u need data base server like to share comment etc.

it is 2-tier application.
backup should be done for time to time at database server.

snapshot and ec2 are concepts of virtualization .
snapshot in virtualization is when creating disks how many files are created and on what time and date it is created all the information is stored and by using this snapshot we can easily backup.


~~~~~~~~~~~~~~~~~~
     SNAPSHOT
~~~~~~~~~~~~~~~~~~
> snapshot -- backup
> informational websites are mostly called static website.
 Ex: google.com,acoe.edu.in etc.
> the application websites where consists login credentials are called Dynamic website.
Ex: Swiggy, Zomato
> there are some websites initially static for information but if we want to check your content we need to login so that will become Dynamic website
> the DATABASE SERVER is one who saves the credentials of a application
> the application server and database server are interlinked with each other (2-tier web application)
> the database server and application server are created in separate EC2 instance
> database server should be protected every time 
> backup should be done for database server
> snapshot and EC2 instances are concepts of Virtualization 
> snapshot enables backup flexibly and in less time
> snapshot in virtualization is when creating disks how many files are created and on what time and date it is created all the information is stored and by using this snapshot we can easily backup.


~~~~~~~~~~~~~~~~~~~~~~~~~~~~
       USING SNAPSHOT
~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Steps:
> Go to Elastic block store select the newly created volume and click 'Create snapshot'.
> Go to snapshot and check whether the snapshot is created or not.
> go to EC2 instance and create directories and write content in it.
> again take the snapshot.
> now Go to Elastic block store select the newly created volume and detach it then delete the volume. 
> go to Elastic block store select the snapshot and select it and in actions create volume from snapshot -> check the availability zone and create snapshot (Don't change volume).
> go to EC2 instance go to command prompt 'cmd' type 'diskmgmt.msc'.
> click the newly created volume and click online.







<<<<<<<<<<<<<< OBJECT STORAGE >>>>>>>>>>>>>>>>

> object storage available in AWS is --> S3 (SSS --> Simple Storage Service)
> You can connect any type of file since it is external storage
> it has unlimited storage
> Inward connections to cloud --> to upload a file into cloud there are no charges in AWS
> Outward connections to cloud --> outgoing connections from cloud are chargeable 

In s3 we have storage tiers
s3 standard -99.99 availability
            -11 9's durability
we use s3 standard for daily storage needs 

any tier in s3 is unlimited storage 

s3 IA --I infrequent Access which means we access data very infrequently for this the charges will be little bit less

s3-ONE ZONE-IA--actually when we store data in server it stored in multiple zones but if u don't want to store in multiple zones as it may be not that much useful the u can go for one zone storage so that cost may be reduced

s3 GLACIER -LESS EXPENSIVE -2HRS cause for retrieving the data it takes minimum 2 hrs as our data is uploaded to cloud and disconnected then we should raise a request and then they process and give the data back.

data is stored in the form of objects in buckets

Though it have unlimited storage no object should go beyond 5tb 
 
s3 bucket names
=>bucket names must be globally unique
=>3-63 characters 
=>use - but not_ 

 
s3 snow family-migrating-sending data from on premises 
s3 snowball 
s3 snowmobile 


==========================================================================================================
10/7/24
==========================================================================================================
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	 	DOWNLOADING OBJECT
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
> Create a bucket in S3 --(sandbox -> search s3 -> create bucket)
> upload files in the bucket 
> select the object and download it 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
		PUBLIC URL OF OBJECT
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
> select the object and go to permissions and 'off' the block public access
> Go to object ownership and select the other rather than recommended on and click the check box and save changes 
> Access control list(ACL) --> edit --> select the 'list' and 'read' checkboxes for everyone and save changes
> go to objects and in actions select 'make public using ACL'
> copy the url and paste it new tab
sx
~~~ 
> this url is given to all public so that others don't need our credentials just to view and download the objects


-------------------------------------------------------------
               bucket versioning
-------------------------------------------------------------
>this versioning keep track on object changes.
>change index file
>upload changed index file and verify in changes
>check versions if available
>enable bucket versioning
>change the index file again
>upload the changed file
>go to versions ,you can see versions
>click on each version to see the changes
>delete the current version to make previous ve22rsion as default

===================================================
=>Aws starts with root like ec2 starts with /
=>root is the main head of all the hierarchy
=>we can create n number of administrator accounts like 
=>storage administrator in which he can work on the buckets, versioning etc.
=>cloud system administrator in which he can work on system resources like ec2 ,elastic beanstalk
=>Basically AWS recommends to create administrators and not to use root for every single thing but use the    administrator account.

API-application programming Interface
by using get-API call the applications are talking each other internally to give a output
cloud only works on API


=>three ways of accessing resources if AWS


Aws management console
Aws cli -command line interface
Aws SDK-software development kit-for python they will use -boto3

=>cloud shell is a predefined or pre authenticated inbuilt bash shell given by the AWS to test code without launching the ec2
=>cloud9, its is a Aws IDE-integrated development environment-ide


=============================================================================================
13/7/24
=============================================================================================
AWS SHARED RESPONSIBILTY MODEL
security is being shared between the two entities they are
1.csp
2.customer

security of the cloud is responsibility of the csp-cloud service provider
security in the cloud is responsibility of the user

IAM-identity and access management
it is a policy document responsible to give permissions to whom and not and it is completely free service


15/7/27
================REAL TIME SCENERIO/PROBLEM===============================

you started your shift and there is an unusually high volume of costumers contacts. you take your first case: a customer operation is critical priority case reporting a slow server where overall service performance is heavily degraded. Specifically, it has gone for full to half speed, You learn that due to their severely degraded service, their customers are having difficulty accessing their online services. you are currently in a chat session with the customer. please note that at Amazon, servers are referred to as instances.
(Simply: system is working very slow. Is there something wrong on the amazon end)


=======================DATABASE SERVER==========================

=>application server-user are clients
=>database server -application server is the 1st client where application server uses the services of the database server and user is 2nd client

=>by making these instances launching differently and connecting to each other there can be some security benefits like database server cant be hacked directly even if the application server is hacked
=>application server-public Ip      ---------|
=>database server-application server    -----|---accessed
=>but not database server-public Ip      ----|

instead of creating two ec2 instances one for database server and one for application server
the AWS offers---->AWS databases-Managed services--->for database 

two types of database servers
1.relational services
2.non-relational services

UNMANAGED SERVICES
deploying ec2 instance using the database server 

ec2-database-unmanaged service 
tables
db software(sql)
OS-customer 
hardware-AWS

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
==>AWS RDS


softwares related to database server which---amazon aurora 
 are provided by aws                         my sql
                                             Microsoft sql server
                                             postgre sql
                                             MariaDB
                                             oracle

this all above process can be done by user so they need to backup everytime


to make all this process easy we have aws rds-relational database service
here everything they do like creating OS, database backup all the thing but we need to create tables and just store the data    this is saas--software


===============
RDS database:-- in sandbox
===============
rds -- databses -- create databse -- standard create -- MySQL or postgre or MariaDB -- templates -- free tier -- settings -- name --username(our wish) -- master password(give separately doesn't include /,'',@) --  vpc(our wish) -- public access -- yes --availability zone(our wish) -- database port:3306


> now install --'download MySQL workbench' from google
> meanwhile try some MySQL commands in mobaxterm :
 1. show databases;
 --> shows the list of databases in the instance
 2. create database <database name>;
 --> creates the database using given name



=============================================================
	Connecting the EC2 instance with ubuntu server
=============================================================
> create a RDS with free tier 
> launch ec2 instance with ubuntu
> NOTE: both the DB and ec2 instance created through default vpc and same availability Zone. 
> update the ubuntu packages (apt-get update) --> in mobaXterm
> install mysql-client(apt-get install mysql-client)
> now go to RDS and select the database and go to connected compute resources
> click 'set up ec2 connection' and click the created instance 
> now scroll up and copy the endpoint URL and go to mobaXterm
> type command --> 'mysql -h <endpoint URL> -u <username in RDS> -p' and click enter then enter password
   > here '-h' is the host
   > '-u' username of the Sql - RDS
   > '-p' password
> Now you will get a 'mysql>' bash
> now install --'download MySQL workbench' from google
> meanwhile try some MySQL commands in mobaxterm :
 1. show databases;
 --> shows the list of databases in the instance
 2. create database <database name>;
 --> creates the database using given name
> After installing the workbench 
> go to RDS and in Security click 'vpc security groups' and click the default one and add edit inbound rules --> 'MySQL/Aurora' -- '0.0.0.0/0' and save changes
> come back to RDS and copy the endpoint url and go to workbench
> click '+' and give 'connection name '--(your wish) and in hostname -- paste the endpoint url 
> After that give the username --> which you gave to RDS and click 'store in vaults' --> enter the password and click ok
> now open the workbench you can see the created databases in 
the schemas






18/7/24


the scalability in RDS is vertical which means increases the processing power
where in non-RDS it is horizontal means increases the number machines

but horizontal scaling is best because if server own all the users wont able to access the application
in vertical even if one system server downs another systems supports.


RDS is managed service we just take care of our application.



29/7/24
=========================================================================================
API&AWS API GATEWAYS
========================================================================================
> postman API --> a website with free API course along with labs

🡲 API works on 'request-response' way , where the user requests the API the API gives the request to the server and takes response and gives it to client.
🡲 Mostly used API requests
1. get-this request cant modify anything
2. put - database cannot be created
3. post - creates database

put updates the existing resources that u give but post creates the database 

=>Based on these request we get response
=>in that response we get the status code like success or not 


REST API-response codes ad statuses---check to know status codes...

for APIs concept check below link
https://apipheny.io/free-api/


30/7/24

=>the request will go to the website ie application that application integrates to the api gateway
=>the user will not directly interact with the API

=========================================================
30/7/24
=========================================================
>API gateway is a proxy
>API gateway is used to rowte the services ,call the services etc..
>API supported protocols:1.Restful -- it is a request / response -- short-lived  -- HTTP related
                         2.Websocket -- it is a two way communication -- long-lived--bidirectional 


-----------------------------------------
creating of API using sandbox:
1.go to  AWS console
2.search for API gateway
3.


1/8/24
==================LAMBDA FUNCION============================
=>lambda---serverless service 
=>it doesn't mean there is no server but the server is launched at the backend by the AWS and it is free of cost
=>they are maintaining big servers for lambda functions to give service for every one they don't charge for the servers being launched but they charge for the instructions being executed by us 
=> it is managed service 
automation, scripts->trigger, particular task

script/code - lambda function
trigger - when to execute-
        -trigger is not only based on the time but also based on the another task


turn on ec2 instance at 10:30 automatically

-->code-- turn on instance 
-->trigger-10:30 pm
      by default AWS will not give permissions to interfere one function to another like lambda to ec2 so hence IAM is used to give permissions
-->IAM role- permission to resource



========================================================================

CONTAINERS

========================================================================



instead of multiple operating systems like in virtualization we use one operating system with multiple applications
         CONTAINERS                            VIRTUALIZATION
          ec2                                   
container      container               |      ec2           ec2
app-email      app-web                 |     app-email     app-web
os                                     |      os            os
hypervisor                             |       hypervisor
hardware                               |        hardware

one virtual machine for 3 servers---i.e. one ec2 instance for 3 servers


containers consists one small lightweight os -- called alpine Linux specially build for containers---its in Mbs like 2mb 3mb some times its in kbs too


----------LAYERS OF CONTAINERS-----------

application---web,email
libraries/binaries(environment)---these will changes for application to application 
os-lightweight os-alpine linux 



dockerhub---container repository----public 

repository---collection of items



containers also solve compatibility issues like developer develops the website using containers and gives to administrator then the administrator will deploy that container to the ec2 instance


Connecting the container and the ec2 instance using mobaxterm
 
1.apt-get update
2.apt-get install docker.io
3.check the installation of docker -- [ docker --version ]
4.In this docker we can see the 
container image-- alpine, lib/bin, application   
container -- running  environment of image
5.TO see the existing or running containers or dockers in the server --[docker ps]
6.TO see any images are downloaded  --[docker images]
7.pulling the container image -- [docker pull httpd] 
which download the images in layers of httpd
8.for every image and container we have a unique id .
9.TO see the running image -- [docker run httpd] -- which runs in a foreground which is not useful for us
10.To stop the container -- [docker stop container id] 
11.To see the stopped containers -- [docker ps -a] 
12.To start the container -- [docker start -d]
13.To remove the container we first stop that container and then we remove it by using this cmd -- [docker rm <containerid>]
14.To remover the image -- [docker rmi <image>]
15.To see the list of the docker container [docker ps]

<<<<<<RUNNING DATABASES IN THE CONTAINERS>>>>>>>>>>>>

16.to create a network--->docker network create networkname[docker network create mongo-network]
17.to see the networks --->docker network ls


18.docker run -d -p 27017:27017 --name mongo --net mongo-network -e MONGO_INITDB_ROOT_USERNAME=admin -e MONGO_INITDB_ROOT_PASSWORD=pass mongo
19.docker logs <hashvalue>

20.docker run -d -p 8081:8081 --name mongo-express --net mongo-network -e ME_CONFIG_MONGODB_ADMINUSERNAME=admin -e ME_CONFIG_MONGODB_ADMINPASSWORD=pass -e ME_CONFIG_MONGODB_SERVER=mongo mongo-express


27017:27017
ec2    container
which means any traffic coming to ec2 instance is forwarded to container by using like above 
generally random name will be given to container if we wanted to give specific name "--name" is used
if we dont give "--net networkname" the container will be runned in the default network
"-e" is for the environment 
=============================
07/07/24
=============================
16.To run the container in the bash -- [docker exec -it container id /bin/bash
17.pwd
18.TO see the environment variables  in the container first run the exec cmd and run env -- [env] 
19.To come out from the container -- [exit]
20.docker run -d httpd -- to create a new container.
21."-d" for detached mode of the container
22."-p" for port number or mapping the port numbers of container and the instance.
23.docker run -d -p80:80 httpd -- The httpd traffic from the ec2 instance is redirected to the port httpd of the container by editing the inbound rules in the ec2 instance.so that the web page works. 
24.docker run -d -p80:8080 httpd -- failed because that port number is already existed for the another instance and the container.
25.docker run -d -p80:8080 MySQL -- it indicates complete.



eg: httpd is image that running environment of image is container

docker run -d httpd---whenever u use this command a new container is created and it is runned in the detached mode


===============================================================
DEPLOYING WEBSITE USIN CONTAINER
===============================================================
> enable the container image with mapping port [docker run -d -p80:80 httpd] and add the inbound rules in the ec2 instance
> upload the website codes in zip file 
Ex: coffee-shop.zip
> [apt-get install zip]
> [unzip 'file.zip'] unzip the file
> docker cp <folders of the website> <containerID>:/usr/local/apache2/htdocs
> load the webpage again, you can see the updated website

======================================================
08/08/24 
======================================================
>containers always listen to the port no:80 that is httpd.

deploying two different websites from two separate containers 
where one container get the traffic from ec2 having port no 80 and another container get the traffic from ec2 having port no 8080.We have to add the inbound rules in the ec2 to deploy a website from the containers.


=============================================================================
Launching Two websites at a time using containers in single EC2 instance
=============================================================================
> in the EC2 instance enable the port name 'http' with port number '80' and as well as enable the port name 'custom TCP' with port number '8080' and save inbound rules

> Now create two directories to copy the website files by using the command [mkdir -p /home/ubuntu/site1 /home/ubuntu/site2]
two directories are created naming 'site1','site2'

> upload the zip files to the instance 

> now unzip the each website files in created directories
[unzip <website zip file> -d /home/ubuntu/site2]

> now create the dockers by using the following command
[# Site 1 on port 80
docker run -d -p 80:80 -v /home/ubuntu/site1:/usr/local/apache2/htdocs httpd

# Site 2 on port 8080
docker run -d -p 8080:80 -v /home/ubuntu/site2:/usr/local/apache2/htdocs httpd]

> verify the container using command
[docker ps]

> open the browser and access your websites
For Site 1: http://<your-ec2-public-ip>:80
For Site 2: http://<your-ec2-public-ip>:8080


13/8/24
==============
load balancer |
==============
it balances the traffic of the users

sample scenario
5000 users--->ec2(application)--cant possible
5000 users--->lb---->ec2(application)
                ---->ec2(application)


load balancer<---traffic<----users
load balancer--->target group

no matter how many instances are there but u have application in two servers of two subnets then those to subnets combinly taken as the target group then the traffic from the users are allowed to the load balancer and that is allowed to the server as fcfs from the load balancer

=================================================================================
primary memory--temporary
secondary memory--hdd--magnetic storage--slow access
                --ssd-semiconductor 

databases--ssd--high iops
3600 iops-
10x iops  --- db ssd

streaming--ott
cache memory ---very expensive---very faster
edge server/pop server ---where the cache is maintained
SRam ---ssd uses this type of memory/cache  l1, l2, l3 
--->DRam


=>the SRAM is very faster also stores data not permanently but 20s-2m


=>using of cache memory in databases the performance of read and write will be increased

=>cache memory is very limited like 100mb cache memory is very expensive

=>because more cache memory more the performance cause every time it is no need to look for database it directly gets information from the cache

==>this is especially used by the applications where the applications are deployed globally like for to search for us type quires which are in us databases but u want faster retrieval then the edge /pop having cache --understands the query and for next time it gives faster retrieval

the service that is available to work with this is aws elasticache
Memcached ---- small apps
Redis ----- more benefits --- high apps like otts

they are in memory datastore but not related to rds
we use this service between the application and database


application--cache---database
application if its is present in cache gives info back else go to database to retrive
if the data is present in cache --->cache hit
if the data is not present in the cache-->cache miss




----------------------------------------------------
Working  with Elastic cache using Redis
----------------------------------------------------
Sandbox -- elastic cache -- get started drop down -- redis oss -- for customization -- select the design your own cache -- cluster cache -- enable(multiple SRAM's are created or nodes are created) -- name  -- description(own)  -- location(aws cloud) -- disable multiAz enable option -- engine version EG:7.0/7.1 doesn't go with the latest versions -- port:6379(must) -- node type(t3 micro) -- replica[0] -- subnet(existing or new one) -- selected subnets(east-1a & 1b) -- next -- security -- check both enable of encrypt at rest & encrypt transit -- access control (select the redis auth default user access) -- password -- disable backups and maintenance -- next -- check the details -- create.





nslookup configurationendpoint  remove: port num---gives the shads which are created in subnets






<<<<<<<<<< Connecting elasticache to ec2 instance >>>>>>>>>>>>

> sandbox - launch Redis - and launch ec2 instance with Linux (version 2 AMI and leave the default changes -- make sure the Redis and ec2 instance has the same vpc connected)-->Go to Redis - network and Security - click modify -- scroll down to security groups and click manage - select the default security group and select modify -- after again go to the network and security - edit inbound rules - delete the ssh and add rule - 'Custom TCP' , Port number-> '6379', source -'select the ec2-instance security group' - save changes - open ec2 instance and select - ec2 instance security group - edit inbound rules - 'Custom TCP' , Port number-> '6379', source - '0.0.0.0/0' and save 
changes - Now launch ec2 instance to mobaxTerm


 
Connecting to the Mobaxterm:
   Type the commands while u are in the user mode:
1.sudo amazon-linux-extras install epel -y

2.sudo yum install gcc jemalloc-devel openssl-devel tcl tcl-devel -y

3.sudo wget http://download.redis.io/redis-stable.tar.gz

4.sudo tar xvzf redis-stable.tar.gz

   check whether the src directory is present in the list:
5.sudo su

6.ls -l

7.change to the redis-stable directory for the following commands

8.cd redis-stable

9.make BUILD_TLS=yes

10.cd src

11.chmod a+x redis-cli

12.cp redis-cli /usr/bin/

     To connect to the redis-cluster type the following commands:

13.redis-cli -h <end point> --tls -a <password> -p 6379

Eg:redis-cli -h clustercfg.redis.43pbiu.use1.cache.amazonaws.com --tls -a token123456789012 -p 6379
  
        If the redis is connected then set the parameters:

14.set <any variable line a-z> "any name"   Eg:set a "test"
 
15.keys * -- to see the parameters          Eg:"a"

16.get <parameter like a-z that we created> Eg:get a




<<<<<<<<<<<< Web Application Firewall >>>>>>>>>>>>>>>>>>

> internet - untrusted network -- the security group doesn't control this untrusted network

> ping is a dangerous command where the request/traffic will continuously asks the application to reply then the server will automatically went down -- it is called 'denial of service'

> Types of attacks
  -> sql injection - launched on databases of application
  -> xss - cross site scripting - uses different patterns to find out the correct password of a user 
  -> dos - ping is a dangerous command where the request/traffic will continuously asks the application to reply then the server will automatically went down -- it is called 'denial of service'
  -> ddos - distributed dos - sending the traffic using ping from different corners of world to a particular web app is called ddos
 
"haveibeenpwned.com" -> to check the breaching of your mail , we can check from which websites our data is breached. i.e. hacking of our emails or credentials from different entities or websites.

> to prevent these types of attacks we need to have the 'web application firewall' -- simply to stop the common attacks on the web application




<< how to add firewall for application using AWS >>

> create vpc - create subnets for the vpc in different subnets - create the internet gateway - attach to vpc - create route table in the vpc u created and click 'edit routes' -> source -'0.0.0.0/0' and select internet gateway and click your internet gateway -- click 'subnet association' and select 'edit subnet association' select the two subnets of the vpc you created and save
> launch two ec2 instances either with Linux or ubuntu in your vpc with different subnets 
> select first ec2 instance and click 'connect'
(For default webpage)
 -> If ubuntu server give commands 
   [apt-get update]
   [apt-get install apache2]
   [systemctl start apache2]
   [cd /var/www/http]
   [cat index.html]
   [vim index.html] --> edit your webpage
 -> If Linux server give commands
   [yum update]
   [yum install httpd]
   [systemctl start httpd]
   [cd /usr/share/httpd/noindex]
   [cat index.html]
   [vim index.html]
(For loading webpage)
 > connect the instances to mobaxterm
   -> If Ubuntu server 
   [apt-get update]
   [apt-get install apache2]
   [systemctl start apache2]
   [chmod 777 /var/www/http]
   [rm -rf /var/www/http/index.html]
   [apt-get install zip]
   Upload the zip file of your website
   [unzip <website zip file> -d /var/www/http]
   [systemctl start apache2]
  -> If Linux server
   [yum update]
   [yum install httpd]
   [systemctl start httpd]
   [chmod 777 /usr/share/httpd/noindex]
   [rm -rf /usr/share/httpd/noindex/index.html]
   [yum install zip]
   Upload the zip file of your website
   [unzip <website zip file> -d /usr/share/httpd/noindex]
   [systemctl start httpd]
> Create the load balancer - create target group for loadbalancer

> search 'Web application firewall'(WAF) in services - from left navigation pane - select 'IP sets' and create IP set - name (your wish) - region(the region should be same as load balancer region)-
Ip address(it can be seen in website 'whatismyip.com')and give subnet - create
> go to Web ACLs in the WAF - click create -name(your wish)- if the application is small select the resources 'regional resources'-'Resource type' --> load balancer and give load balancer name - Rule Type - name - IP set --> select the IP set you created
> cloudfront is a service which protects, also gives good performance of the app and monitors the web app . It is especially used for globally distributed applications


==== HW =========|
1. service name  | 
service category | 
Description      | 
                 | 
2. port numbers  |
_________________|


===========================SCALING=====================
the no of servers to be increased/decreased and capacity of the server is to be increased/decreased
upgrading of resources 

scaling is of two types ---horizontal---increasing the no of servers
                        ---vertical--one server but increasing the capacity of the server


Depend on the traffic coming to the server the servers are increased/decreased automatically without any manual power so that it gives the good performance is called autoscaling or creating multiple replications of the same instance is called autoscaling


in the autoscaling whenever the instances are running the also creating the replications of the base instance is based on the image of the instance only not from the existing instance  

Autoscaling--AWS SNS--Simple Notification Service
Which notify about the events to the subscribers


========================================================
28/08/24          
=======================================================
1.launch the ec2 instance with application
2.create image
3.create SNS topic and subscription
4.create autoscaling launch configuration / launch template
5.create the new load balancer and target group
6.Attach SNS topic to autoscaling
7.Define the desired /min /max capacity
8.Create autoscaling
9.install stress in one of the server
10.create autoscaling
11.install stress in one of the server
12.check the autoscaling functionality


aws.skillbuild.com
aws associate practice 
aws certified developer -associate (dva-c02)


exam topics ---dump---most repeated questions




aws cloudformation -provides infrastructure as a code 






Hybrid cloud
SaaS
Elasticity
Durability
Availability
AWS Global Infrastructure
IAM policy
IAM Role
AWS Shared Responsibility Model
VPC and VPC components
EC2
LAMBDA
Elastic Beanstalk
Elastic Block Storage
s3
S3 Glacier
RDS
DynamoDB
Elastic Loadbalancing
Auto Scaling
Amazon CloudWatch
AWS SDK
AWS CLI
API Gateway
Docker container
AWS ECR
Elasticache
Redis
SNS
Step Function



A developer of X company needs a ubuntu updated server to deploy a company related web application. As per the company policy the key files should not be shared to any user of organization. The cloud administrator is tasked to provide the access to developer for a ubuntu ec2 instance fulfilling above requirements.


====================================================
		   NAT GATEWAY
====================================================
> one-way communication and a secured connection
> It is used to give connection from public EC2(bastion host) to private EC2 others cannot to this while there is a connection already established
============================================================
1.SSL -- secured socket layer -- CA -- certificate authority(provides the ssl certificate and install the server)
2.confidentially through encryption -- the entire conversation will be encrypted through ssl certificate. 
3.The conversation between the two servers/computers is in plain text where the hackers can easily understand.
4.http -- 80 -- plain text
5.All the web servers / conversations happening with https for security.
6."letsencrypt" and "openssl" are the open sources which give the ssl certificates
7.CA -- pay -- ssl -- apply from godaddy or google 
private key generated by the user in the server i.e " .pem "
where the certificate is generated by the CA which ends with " .crt " extension .It takes minimum 2 hours to generate.
Here pem file and crt file are interlinked .
8.ssl ---> .pem ,.crt
it takes minimum 1 hour to reflect.
9.standalone server - also called ssl termination
We install the ssl in server itself where the server is the standalone server / single server.
mywebsite.com --> ip address -->ssl --->https 
             DNS provider
10.The two files are terminated when they are present in a single server.
11.they are also terminated when there are multiple servers in the load balancer.
[apt-get install certbot python3-certbot-apache] --> to obtain the certificate we use this command







<<<<<<RUNNING DATABASES IN THE CONTAINERS>>>>>>>>>>>>

16.to create a network--->docker network create networkname[docker network create mongo-network]
17.to see the networks --->docker network ls


18.docker run -d -p 27017:27017 --name mongo --net mongo-network -e MONGO_INITDB_ROOT_USERNAME=admin -e MONGO_INITDB_ROOT_PASSWORD=pass mongo
19.docker logs <hashvalue>

20.docker run -d -p 8081:8081 --name mongo-express --net mongo-network -e ME_CONFIG_MONGODB_ADMINUSERNAME=admin -e ME_CONFIG_MONGODB_ADMINPASSWORD=pass -e ME_CONFIG_MONGODB_SERVER=mongo mongo-express


27017:27017
ec2    container
which means any traffic coming to ec2 instance is forwarded to container by using like above 
generally random name will be given to container if we wanted to give specific name "--name" is used
if we dont give "--net networkname" the container will be runned in the default network
"-e" is for the environment 



<<<<<<<<<VERSION CONTROLLING SYSTEM>>>>>>>>>>>>
tracking the version/code is called version controlling.

> git -is a version controlling system
> remote platforms means once the code is uploaded that can be pulled used from any where of the world
> remote git platforms --GitHub, GitLab
> local platforms means we can create repositories locally and can access locally
> using GitHub, GitLab we can create ci/cd pipe lines 
> repository-one central directory which contains all the project files
> git client-->git bash(Linux based commands)
> 






